\documentclass[12pt, oneside]{article}
\usepackage{a4wide}
\usepackage{oldgerm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{tikz}
\usepackage{multirow}
\usetikzlibrary{automata, positioning}
\setlength{\textheight}{8.875in} \setlength{\textwidth}{6.875in}
\setlength{\columnsep}{0.3125in} \setlength{\topmargin}{0in}
\setlength{\headheight}{0in} \setlength{\headsep}{0in}
\setlength{\parindent}{1pc} \setlength{\oddsidemargin}{-.304in}
\setlength{\evensidemargin}{-.304in}



\begin{document}
\setlength{\textheight}{8.5in}
\centering {\bf MTL 106 (Introduction to Probability Theory and Stochastic Processes) }\\


\centering{\bf Assignment 2 Report}



\vskip 0.5cm

\noindent Name: Arpit Saxena ~~~~~~~~~~~~~~~~~~~~~ Entry Number: 2018MT10742



\vskip 0.5cm



\begin{enumerate}
	




\item {
    Basic Probability

    Let \(\Omega = \{0, 1, 2, 3, 4\}\) be the sample space. Consider the \(\sigma\)-field
    \[\mathcal{F} = \{\varnothing, \{0\}, \{1\}, \{0, 1\}, \{0, 2, 3, 4\}, \{1, 2, 3, 4\}, \{2, 3, 4\}, \Omega\}\]
    and a function P from \(\mathcal{F}\) to \([0, 1]\) with \(P(\{0\}) = 0.4\),
    \(P(\{1\}) = 0.3\) and \(P(\{2, 3, 4\}) = x\). Find the value of \(x\) such that \(P\)
    is a probability on \((\Omega, \mathcal{F})\). Find all the values of this function.

    \textbf{Answer}

    Since we have that \(P(\Omega) = 1\) and \(\{0\}, \{1\}, \{2, 3, 4\}\) are disjoint
    events whose union is \(\Omega\), we have
    \begin{align*}
        P(\{0\}) + P(\{1\}) + P()\{2, 3, 4\}) &= 1 \\
        \implies 0.3 + 0.4 + x &= 1 \\
        \implies x &= 0.3
    \end{align*}

    Using this, we can calculate the value of \(P\) for all other elements in \(\mathcal{F}\)
    \begin{align*}
        P(\{0, 1\}) &= P(\{0\}) + P(\{1\}) = 0.7 \\
        P(\{0, 2, 3, 4\}) &= 1- P(\{1\}) = 1 - 0.3 = 0.7 \\
        P(\{1, 2, 3, 4\}) &= 1 - P(\{0\}) = 1 - 0.4 = 0.6 \\
        P(\{2, 3, 4\}) &= 1 - P(\{0, 1\}) = 1 - 0.7 = 0.3
    \end{align*}
}

\item {
    Random Variable/Function of a Random Variable

    Alice is trying to send data to Bob using the Hamming codes for error resilience.
    There is \(10\%\) chance for each bit to flip during flip. Let \(X\) be a random
    variable with distribution \(P(5)\). \(2^X - 1\) bits are sent, and when they
    are received, Bob takes an xor of the binary representation of all the positions in
    which the bits are set (after arranging them in a square with positions 0 to \(2^X - 1\)
    and starting arranging data from position 1) and if it is 0, there is no error, 
    otherwise there is an error.
    Find the probability that given \(\leq 3\) bits are flipped in transmission, the error
    is not detected by Bob.

    \textbf{Answer}

    When the data is sent, the xor of all the positions in which bits are set must be
    0, since that is the requirement of Hammond codes. So, the xor of the location of all the
    bits which flip would be the calculation that Bob makes.

    We have 4 cases here: that of 1, 2, 3 bits flipping in transmission
    \begin{enumerate}
        \item {
            1 bit flips

            Here, the location of the bit that flip would appear as the result of the xor
            operation. Since the bits are all placed in non zero locations, the result
            would be non zero. Hence such an error would be detected with certainty.
        }
        \item {
            2 bits flip

            Suppose two bits in distinct locations flip. If Bob were to not be able to
            detect them, the xor of the location of both these bits would be 0.
            But \(A \oplus B = 0 \implies A = B\) which contradicts our assumption.
            Thus the result would be non-zero and be detected by Bob with complete
            certainty.
        }
        \item {
            3 bit flips

            Suppose 3 bits flip. Since the address of has \(X\) bits, then at each of
            the positions, xor should be zero for Bob to not be able to detect it.

            Thus, if \(A_{X-1}\ldots A_{1}A_{0}\), \(B_{X-1}\ldots B_{1}B_{0}\) and
            \(C_{X-1}\ldots C_{1}C_{0}\) represent the addresses of the 3 bits which
            flipped, for the error to go undetected, we must have:
            \[A_i \oplus B_i \oplus C_i = 0, i = 0, \ldots, X - 1\]

            For a single \(i\), from the total number of possibilities, there are
            \({3 \choose 2} + 1 = 4\) possibilities of error going undetected, either two
            of them could be equal to 1 and the other is 0, or all 3 bits could be 0.

            Since these are independent, there are \(4^X - 1\) possibilities for error
            to go undetected (1 is subtracted since we don't have the address containing
            all zeroes). Total number of 3 bit errors possible is \({2^X - 1} \choose 3\)
            Thus probability of an error going undetected given 3 bits were flipped is
            \[\frac{6(2^X + 1)}{(2^X - 2)(2^X - 3)}\]
        }
    \end{enumerate}

    Now, we have:
    \begin{align*}
        P(\text{Error going undetected} | \leq 4 \text{ bits flipped}) &=
            \frac{P(\text{Error going undetected}, \leq 4 \text{ bits flipped})}{P(\leq 4 \text{ bits flipped})} \\
            \intertext{Now using total probability law, we have}
            P(\text{Error going undetected}, \leq 4 \text{ bits flipped}) &=
                P(\text{Undetected}, 1 \text{ bit flipped}) \\
                &+ P(\text{Undetected}, 2 \text{ bits flipped}) \\
                &+ P(\text{Undetected}, 3 \text{ bits flipped}) \\
                &= 0 + 0 + \frac{6(2^X + 1)}{(2^X - 2)(2^X - 3)} \\
        P(\leq 4 \text{ bits flipped}) &= 0.9^{2^X-1} + {2^X \choose 1}0.1\cdot0.9^{2^X - 2} \\
                                    &+ {2^X \choose 2}0.1^2\cdot0.9^{2^X - 3} + {2^X \choose 3}0.1^3\cdot0.9^{2^X - 4} \\
        \therefore P(\text{Error going undetected} | \leq 4 \text{ bits flipped})
            &= \frac{6(2^X + 1)}{(2^X - 2)(2^X - 3)} \cdot \frac{1}{P(\leq 4 \text{ bits flipped})}
    \end{align*}
}


\item {
    Stochastic Processes

    For a game theory demonstration, people are asked to pick any number between 0 to 100
    (both inclusive) and the one whose guess is closest to two-thirds of the average wins.
    However, due to a mistake, people guess one by one. After \(n\) people have guessed,
    the next person to guess chooses a random number between 1 to two-thirds of the running
    average. The first person to guess chose a number uniformly between 1 to 100.

    Model the running average as a stochastic process and find out which properties 
    it satisfies. What is the expected running average as more and more people guess?

    \textbf{Answer}

    Let \(X_i\) be the random variable denoting the guess made by the \(i\)'th person.
    Define \(S_i\) as:
    \[
        S_i = \begin{cases}
            0 & i = 0 \\
            S_{i-1} + \frac{X_i - S_{i - 1}}{i} & i = 1, 2, \ldots
        \end{cases}
    \]

    Then \(S_i\) is the running average after \(i\) people have guessed.
    
    As per the information given in the question, \(X_i \sim U(0, 100)\)

    For \(i = 2, 3, \ldots\), we have been given that
    \[X_i / S_{i - 1} = s \sim U\left(0, \frac{2s}{3}\right) \]
    \begin{align*}
        S_1 &= X_1 \\
        \implies E(S_1) &= E(X_1) = \frac{0 + 100}{2} = 50 \\
    \end{align*}

    Now, for \(n \geq 2\),
    \begin{align*}
        E(X_n) &= E(E(X_n / S_{n-1})) \\
               &= E\left(\frac{S_{n-1}}{3}\right) \tag*{since \(X_n / S_{n-1} \sim U\left(0, \frac{2S_{n-1}}{3}\right)\)}\\
               &= \frac{E(S_{n-1})}{3} \\
        \implies E(S_n) &= E\left(S_{n-1} + \frac{X_n - S_{n-1}}{n}\right) \\
                        &= E(S_{n-1})\left(1 - \frac{1}{n}\right)+ \frac{E(X_n)}{n} \\
                        &= E(S_{n - 1}) \left(1 - \frac{2}{3n}\right) \\
                        &= E(S_1) \prod_{i = 0}^{n-1}\left(1 - \frac{2}{3(n-i)}\right) \\
    \end{align*}

    We note that \(n - i \leq n \,,\, i = 0, 1, \ldots, n-1\), which gives us 
    \(1 - \frac{2}{3(n-i)} \leq 1 - \frac{2}{3n} < 1\). This implies that 
    \(\prod_{i = 0}^{n-1}\left(1 - \frac{2}{3(n-i)}\right) \leq \left(1 - \frac{2}{3n}\right)^n\).
    From this, we can see that \(E(S_n) \to 0 \text{ as } n \to \infty\), which means that the
    expected running average goes to zero as more and more people guess.

    We check for the following properties for this process:
    \begin{enumerate}
        \item {
            Independent Increments

            Since the numbers guessed by the people influence the running average on which 
            the next people make their decision, the increments are not independent. An
            increment (i.e. a person's guess)  depends on the sum of the guessed of the people
            who came before him.
        }
        \item {
            Wide-sense stationarity

            We have shown that \(E(S_n) = \left(\frac{2}{3}\right)^{n-1} 50\), which is a function
            of \(n\), thus this stochastic process is not wide-sense stationary.
        }
        \item {
            Strict-sense stationarity 

            The process does not satisfy weak-sense stationarity property and hence does not
            satisfy strong sense stationarity property since it is a stronger form.
        }
        \item {
            Markov property

            The process satisfies Markov property as \(S_n\) only depends on \(S_{n-1}\)
            and \(X_{n}\), and not on the previous \(S_i\)'s and thus the process
            satisfies Markov property
        }
    \end{enumerate}
}

\item {
    Stochastic Processes

    Consider the process \(X(t) = At + \frac{B}{t} \,,\, t > 0\) where A and B are iid random variables
    with mean 0 and variance 1. Is \(\{X_t : t > 0\}\) wide-sense stationary?

    \textbf{Answer}

    We need to check 3 things to check for wide-sense stationarity which are checked
    as follows:
    \begin{enumerate}
        \item {
            \(E(X(t)\) is not a function of \(t\)

            Here, 
            \begin{align*}
                E(X(t)) &= E\left(At + \frac{B}{t}\right) \\
                        &= t E(A) + \frac{E(B)}{t} \tag*{(Using linearity of expectation)} \\
                        &= 0 \tag*{(Since E(A) = E(B) = 0 is given)}
            \end{align*}
        }
        \item {
            \(E(X^2(t) < \infty)\)

            In this case, we have \(E(A^2) = var(A) + [E(A)]^2 = 1 = E(B^2)\), then
            \begin{align*}
                E(X^2(t)) &= E\left(A^2t^2 + \frac{B^2}{t^2} + 2AB\right) \\
                \intertext{Now using linearity of expectations, \(E(A^2) = 1 = E(B^2)\) and \(E(AB) = E(A)E(B) = 0\)}
                E(X^2(t)) &= t^2 + \frac{1}{t^2} < \infty \,\forall\, t > 0
            \end{align*}
        }
        \item {
            \(Cov(X(t), X(s))\) depends only on \(|t - s|\)

            \begin{align*}
                Cov(X(t), X(s)) &= E(X(t)X(s)) - E(x(t))E(X(s)) \\
                                &= E\left(A^2st + \frac{B^2}{st} + AB\frac{t}{s} + AB\frac{t}{s}\right) \\
                                &= st + \frac{1}{st}
            \end{align*}

            Thus we observe that \(Cov(X(t), X(s))\) is not a function of only \(|s - t|\)
        }
    \end{enumerate}

    Thus, the given stochastic process is not wide sense stationary.
}

\item {
    DTMC

    \begin{enumerate}
        \item {
            The transition matrix \(P\) of a Discrete Time Markov Chain is given as:
            \[P = \begin{pmatrix}
                0.6 & 0.2 & 0.1 & 0 & 0 & 0.1 & 0 & 0 \\
                0.1	& 0.1 & 0.2 & 0.2 & 0 & 0 & 0.4 & 0 \\
                0.2 & 0.3 & 0 & 0 & 0 & 0.1 & 0.2 & 0.2 \\
                0.5 & 0.2 & 0.1 & 0.1 & 0 & 0.1 & 0 & 0 \\
                0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.1 & 0.3 \\
                0.3 & 0 & 0.3 & 0.2 & 0.1 & 0 & 0 & 0.1 \\
                0 & 0 & 0 & 0 & 0.5 & 0.4 & 0 & 0.1 \\
                0 & 0.1 & 0.1 & 0.1 & 0 & 0.2 & 0.2 & 0.3
            \end{pmatrix}\]
            Without calculating the stationary probabilities, find out whether stationary
            distribution exists for this system or not.
        }
        \item {
            Consider a DTMC of \(n\) states. We know that the transition matrix for the
            DTMC is diagonalisable and absolute value of all of its eigenvalues is less than 1.
            Will the limiting distribution of this system exist?

            Use the result to find the existence of limiting distribution of a DTMC with
            the following transition matrix:
            \[\begin{pmatrix}
                0 & \frac{2}{3} & \frac{1}{3} \\
                \frac{1}{2} & 0 & \frac{1}{2} \\
                \frac{1}{2} & \frac{1}{2} & 0
            \end{pmatrix}\]
        }
    \end{enumerate}

    \textbf{Answer}

    \begin{enumerate}
        \item {
            Let \(\pi\) be the stationary probabilities for the system. i.e.
            \(\pi = (\pi_0, \pi_1, \ldots, \pi_{n-1})\)

            Also, we have \(\pi P = \pi\) since the distribution is taken as stationary
            \begin{align*}
                \pi P &= \pi \\
                \implies \pi (P - I) &= 0 \\
                \intertext{Thus we have a homogenous system of linear equations. Since we are
                after a non-trivial solution (as row sum of \(\pi\)= 1), we have the condition}
                det(P - I) &= 0
            \end{align*}

            Note that we have an overconstrained system of linear equations, given by
            \(\pi P = \pi, \sum_{i=0}^{n-1} \pi_i = 1\) along with the condition that
            \(\pi_i \geq 0 \,\forall\, i = 0, 1, \ldots, n - 1\)

            Thus, the condition that \(det(P - I) = 0\) is necessary for stationary distribution
            to exist but not sufficient.
            
            For the given transition matrix:
            \begin{align*}
                \det(P - I) &= \begin{vmatrix}
                    -0.4 & 0.2 & 0.1 & 0 & 0 & 0.1 & 0 & 0 \\
                    0.1	& -0.9 & 0.2 & 0.2 & 0 & 0 & 0.4 & 0 \\
                    0.2 & 0.3 & -1 & 0 & 0 & 0.1 & 0.2 & 0.2 \\
                    0.5 & 0.2 & 0.1 & -0.9 & 0 & 0.1 & 0 & 0 \\
                    0.1 & 0.1 & 0.1 & 0.1 & -0.9 & 0.1 & 0.1 & 0.3 \\
                    0.3 & 0 & 0.3 & 0.2 & 0.1 & -1 & 0 & 0.1 \\
                    0 & 0 & 0 & 0 & 0.5 & 0.4 & -1 & 0.1 \\
                    0 & 0.1 & 0.1 & 0.1 & 0 & 0.2 & 0.2 & -0.7
                \end{vmatrix} \\
                        &\neq 0
            \end{align*}

            This tells us that the given DTMC will not have a stationary distribution.
        }
        \item {
            Let \(P\) denote the transition probability matrix for the DTMC. Then we know
            that \(P\) is a \(n \times n\) matrix. Given that it is diagonalisable,
            \(\exists \text{ invertible } A_{n \times n}\) such that \(P = A \Lambda A^{-1}\)
            where
            \[\begin{pmatrix}
                \lambda_1 & 0         & \dots  & 0 \\
                0         & \lambda_2 & \dots  & 0 \\
                \vdots    &           & \ddots & \vdots\\
                0         & \dots     & 0      & \lambda_n 
            \end{pmatrix}\]
            where \(\lambda_1, \ldots, \lambda_n\) are the eigenvalues of the matrix \(P\).
            We are given that \(|\lambda_i| < 1, i = 1, 2, \ldots, n  \)
            
            \begin{align*}
                P &= A \Lambda A^{-1} \\
                \implies P^2 &= A \Lambda A^{-1} A \Lambda A^{-1} \\
                             &= A \Lambda ^ 2 A^{-1} \\
                \intertext{Continuing like this, we get}
                P^m &= A \Lambda ^ m A^{-1} \\
            \end{align*}

            Since \(\Lambda\) is a diagonal matrix, \(\Lambda^m\) can be easily calculated and is given
            by:
            \begin{align*}
                \Lambda^m &= \begin{pmatrix}
                    \lambda_1^m & 0         & \dots  & 0 \\
                    0         & \lambda_2^m & \dots  & 0 \\
                    \vdots    &           & \ddots & \vdots\\
                    0         & \dots     & 0      & \lambda_n^m
                \end{pmatrix} \\
                \implies \lim_{m \to \infty} \Lambda^m &= \begin{pmatrix}
                    \lim_{m \to \infty}\lambda_1^m & 0         & \dots  & 0 \\
                    0         & \lim_{m \to \infty} \lambda_2^m & \dots  & 0 \\
                    \vdots    &           & \ddots & \vdots\\
                    0         & \dots     & 0      & \lim_{m \to \infty} \lambda_n^m
                \end{pmatrix} \\
                \intertext{\(|\lambda_i| \leq 1 \implies \lim_{m \to \infty}\lambda_i^m = 0\) thus we have}
                \lim_{m \to \infty} \Lambda^m &= 0 \\
                \implies \lim_{m \to \infty} P^m &= \lim_{m \to \infty} A \Lambda^m A^{-1} = 0
            \end{align*}

            Thus, the \(m\)-step transition probability matrix \(P^{(m)} = P^m\) goes to
            0 as \(m \to \infty\) which implies that the limiting distribution does not
            exist.

            Now, given the transition matrix 
            \[\begin{pmatrix}
                0 & \frac{2}{3} & \frac{1}{3} \\
                \frac{1}{2} & 0 & \frac{1}{2} \\
                \frac{1}{2} & \frac{1}{2} & 0
            \end{pmatrix}\]
            
            The eigenvalues of the given matrix are \(\pm \frac{1}{2}, 1\).
            Since it has 3 distinct eigenvalues, it must be diagonalisable.
            Two diagonal entries of the eigenvalue matrix will go to zero, and the other
            would be 1. Thus \(\lim_{m \to \infty} P^m\) exists and thus the limiting 
            distribution for the given DTMC exists.
        }
    \end{enumerate}
}


\item {
    DTMC

    Quantum computers can be modelled using Quantum automata, which are a quantum analog
    of probabilistic automata. A finite probabilistic automaton consists of a set of states
    and the probability of transition from one state to the other depends on the state itself
    as well as the input symbol. Consider an automata with input symbol set as \(\{0, 1\}\)
    and the transition probabilities as:
    \begin{enumerate}
        \item {
            For input 0:
            \[\begin{pmatrix}
                0.5 & 0.5 & 0 \\
                0 & 0.5 & 0.5 \\
                0.5 & 0 & 0.5
            \end{pmatrix}\]
        }
        \item {
            For input 1:
            \[\begin{pmatrix}
                0 & 0.5 & 0.5 \\
                0.5 & 0 & 0.5 \\
                0.5 & 0.5 & 0
            \end{pmatrix}\]
        }
    \end{enumerate}
    The probability for states 0, 1, 2 to accept are 0.3, 0.6, 0.2 respectively.
    What is the probability that the automata accepts the string 0101? Assume the starting
    state is 0.

    Note: While this is not a DTMC in itself, it is a very useful extension of the 
    concept and has wide ranging applications in quantum computing, machine learning, etc.

    \textbf{Answer}

    We can tabulate the states the automata might be in along with the probabilities.

    \begin{center}\begin{tabular}{| c | c | c | c |}
        \hline
        Input & P(state 0) & P(state 1) & P(state 2) \\
        \hline
        Initial & 1 & 0 & 0 \\
        \hline
        0 & \(0.5 \times 1 = 0.5\) & \(0.5 \times 1 = 0.5\) & \(0 \times 0 = 0\) \\
        \hline
        1 & \(0.5 \times 0.5 + 0.5 \times 0 \) & \(0.5 \times 0.5 + 0.5 \times 0\) & \(0.5 \times 0.5 + 0.5 \times 0.5\) \\
          & \(= 0.25\) & \(= 0.25\) & \(= 0.5\) \\
        \hline
        0 & \(0.5 \times 0.25 + 0.5 \times 0.25\) & \(0.5 \times 0.25 + 0.5 \times 0.5\) & \(0.5 \times 0.25 + 0.5 \times 0.5\) \\
        0 & \(= 0.25\) & \(= 0.375\) & \(= 0.375\) \\
        \hline
        1 & \(0.5 \times 0.375 + 0.5 \times 0.375\) & \(0.5 \times 0.25 + 0.5 * 0.375\) & \(0.5 \times 0.25 + 0.5 \times 0.375\) \\
        1 & \(= 0.375\) & \(= 0.3125\) & \(= 0.3125\) \\
        \hline
    \end{tabular}\end{center}

    Taking into account the acceptance probability, we get the probability that the given
    string is accepted by the automata as 
    \(0.375 \times 0.3 + 0.3125 \times 0.6 + 0.3125 \times 0.2 = 0.3625\)
}

\item {
    CTMC

    Consider a  modified version of Conway's game of life. In this simplified version,
    we model using a birth death process where the maximum number of entities allowed in
    the system is n. When the number of entities in the system is \(i\), the birth rate is
    given as \(1 - \frac{i}{n}\) and the death rate is given as \(\frac{i}{n}\). These
    model the qualities that birth rate decreases with increasing population and death
    rate increases due to overpopulation.

    Give the diagrammatic representation of the process. Find the steady state distribution
    of the system, and the expected number of entities in steady state.

    \textbf{Answer}

    The diagrammatic representation is given below:

    \begin{center}\begin{tikzpicture}
        \node[state] (0) {0};
        \node[state, right=of 0] (1) {1};
        \node[state, right=of 1] (2) {2};
        \node[right=of 2] (dots1) {\(\cdots\)};
        \node[state, right=of dots1] (i) {i};
        \node[right=of i] (dots2) {\(\cdots\)};
        \node[state, right=of dots2] (n) {n};

        \draw[every loop]
            (0) edge[bend left, auto=left] node {1} (1)
            (1) edge[bend left, auto=left] node {\(1 - \frac{1}{n}\)} (2)
            (2) edge[bend left, auto=left] node {} (dots1)
            (dots1) edge[bend left, auto=left] node {} (i)
            (i) edge[bend left, auto=left] node {\(1 - \frac{i}{n}\)} (dots2)
            (dots2) edge[bend left, auto=left] node {\(1 - \frac{n - 1}{n}\)} (n)
            

            (2) edge[bend left, auto=left] node{\(\frac{2}{n}\)} (1)
            (1) edge[bend left, auto=left] node{\(\frac{1}{n}\)} (0)
            (dots1) edge[bend left, auto=left] node{} (2)
            (i) edge[bend left, auto=left] node{\(\frac{i}{n}\)} (dots1)
            (dots2) edge[bend left, auto=left] node{} (i)
            (n) edge[bend left, auto=left] node{\(1\)} (dots2);
    \end{tikzpicture}\end{center}

    Let \(X_t\) denote the number of entities in the system at time \(t\).
    Let \(\pi\) denote the stationary probabilities of the system, i.e.,
    \(\pi = (\pi_0, \pi_1, \ldots, \pi_n)\) where \(\pi_i = \lim_{t \to \infty} P(X_t = i)\)

    Using the general formula for a birth death process, we have
    \begin{align*}
        \pi_i &= \frac{\lambda_0 \lambda_1 \ldots \lambda_{i-1}}{\mu_1 \mu_2 \ldots \mu_i} \pi_0 \\
              &= \frac{1(1 - \frac{1}{n})\ldots(1 - \frac{i - 1}{n})}{\frac{1}{n}\frac{2}{n}\ldots\frac{i}{n}} \pi_0 \\
              &= \frac{n(n-1)\ldots(n - i + 1)}{i!} \pi_0\\
              &= \frac{n!}{i! (n - i)!} \pi_0 \\
        \implies \pi_i &= {n \choose i} \pi_0 \,,\,i = 1, 2, \ldots, n
    \end{align*}

    Now we use the normalising condition:
    \begin{align*}
        \sum_{i = 0}^{n} \pi_i &= 1 \\
        \implies \pi_0 + \sum_{i = 1}^{n} \pi_i &= 1 \\
        \implies \pi_0 + \sum_{i = 1}^{n} {n \choose i} \pi_0 &= 1 \\
        \implies \sum_{i = 0}^{n} {n \choose i} \pi_0 &= 1 \\
        \implies 2^n \pi_0 &= 1 \\
        \implies \pi_0 &= 2^{-n} \\
        \implies \pi_i &= {n \choose i} 2^{-n} \,,\, i = 0, 1, \ldots n
    \end{align*}

    Let \(X\) be the random variable denoting the number of entities in the system in
    steady state. Then \(P(X = i) = \pi_i \,,\, i = 0, 1, \ldots, n\)

    \begin{align*}
        E(X) &= \sum_{i = 0}^n i \pi_i \\
             &= \sum_{i = 0}^n i {n \choose i} 2^{-n} \\
             &=  2^{-n} \sum_{i = 0}^n i {n \choose i}
    \end{align*}

    To calculate \(\sum_{i = 0}^n i {n \choose i}\) we use the binomial formula as:
    \begin{align*}
        (1 + x)^n &= \sum_{i = 0}^{n} {n \choose i} x^i \\
        \intertext{Differentiating wrt \(x\) on both sides, we get}
        n (1 + x)^{n - 1} &= \sum_{i = 1}^{n} i {n \choose i} x^{i - 1} \\
        \intertext{Now putting x = 1 implies}
        n 2^{n-1} &= \sum_{i = 1}^n i {n \choose i} \\
        n 2^{n-1} &= \sum_{i = 0}^n i {n \choose i}
    \end{align*}

    Substituting in the above equation, we get
    \[E(X) = 2^{-n} \cdot n 2^{n - 1} = \frac{n}{2}\]
}

\item {
    CTMC

    Consider a Markov process where states are \(0, 1, 2, \ldots\) and any state \(i\)
    can move to state \(i + 1\) and \(i + 2\) both with rate \(\lambda\). Also, a state
    \(i\) can move to \(i - 1\) with rate \(\mu\). Give a diagrammatic representation of
    the process. Find the expected state in which the process would be in steady state

    \textbf{Answer}

    The diagrammatic representation of the process can be shown as:

    \begin{center}
    \begin{tikzpicture}
        \node[state] (0) {0};
        \node[state, right=of 0] (1) {1};
        \node[state, right=of 1] (2) {2};
        \node[right=of 2] (dots) {\(\cdots\)};

        \draw[every loop]
            (0) edge[bend left, auto=left] node {\(\lambda\)} (1)
            (0) edge[bend left, auto=left] node {\(\lambda\)} (2)
            (1) edge[bend left, auto=left] node {\(\lambda\)} (2)
            (1) edge[bend left, auto=left] node {\(\lambda\)} (dots)
            (2) edge[bend left, auto=left] node {} (dots)
            
            (dots) edge[bend left, auto=left] node{} (2)
            (2) edge[bend left, auto=left] node{\(\mu\)} (1)
            (1) edge[bend left, auto=left] node{\(\mu\)} (0);
    \end{tikzpicture}
    \end{center}

    At steady state, assume the probability vector to be \(\pi\), then using the Kolmogorov
    forward equations and setting rate of change of P to be 0, we get
    \begin{align*}
        0 &= -2\lambda\pi_0 + \mu \pi_1 \\
        0 &= \lambda\pi_0 + \mu\pi_2 - (2\lambda + \mu)\pi_1 \\
        0 &= \lambda\pi_{i-2} + \lambda\pi_{i-1} + \mu\pi_{i+1} - (2\lambda + \mu)\pi_i
    \end{align*}

    Let us define \(P(x) = \sum_{n = 0}^{\infty} \pi_n x^n\). Then, we have
    \begin{align*}
        P(x) &= \pi_0 + \pi_1 x + \pi_2 x^2 + \cdots + \pi_{i-2}x^{i-2} + \cdots \\
        \lambda x^3 P(x) &= \lambda \pi_0 x^3  + \lambda \pi_1 x^4 + \lambda \pi_2 x^5+ \cdots + \lambda\pi_{i-2}x^{i+1} + \cdots \\
        \lambda x^2 P(x) &= \lambda \pi_0 x^2 + \lambda \pi_1 x^3 + \lambda \pi_2 x^4 + \cdots + \lambda\pi_{i-1}x^{i+1} + \cdots \\
        -(2\lambda + \mu)x P(x) &= -(2\lambda + \mu) \pi_0 x -(2\lambda + \mu) \pi_1 x^2 - \cdots
                                   -(2\lambda + \mu)\pi_{i}x^{i+1} + \cdots \\
        \mu P(x) &= \mu\pi_0 + \mu\pi_1 x + \mu\pi_2 x^2 + \cdots + \mu\pi_{i+1}x^{i+1} + \cdots \\
        \intertext{Adding the last 4 equations and using the equations obtained earlier we get}
        \left\{\lambda x^3 + \lambda x^2 - (2\lambda + \mu)x + \mu\right\}&P(x) = \mu \pi_0 - \mu \pi_0 x \\
        \implies P(x) &= \frac{\mu \pi_0 (1 - x)}{\lambda x^3 + \lambda x^2 - (2\lambda + \mu)x + \mu} \\
        \implies P(x) &= \frac{\mu \pi_0 (1 - x)}{(x - 1)(\lambda x^2 + 2 \lambda x - \mu)} \\
        \implies P(x) &= \frac{-\mu \pi_0 }{\lambda x^2 + 2 \lambda x - \mu}
    \end{align*}

    Using the definition of \(P(x)\), we get \(P(1) = \sum_{n = 0}^{\infty} \pi_n = 1\) and 
    let \(X\) be the distribution in steady state, then \(P'(1) = \sum_{n = 0}^{\infty} n \pi_n = E(X)\) 

    Using these conditions, we have 
    \begin{align*}
        P(1) &= 1 \\
        \implies \frac{-\mu \pi_0 }{3 \lambda - \mu} &= 1 \\
        \implies \pi_0 &= 1 - \frac{3 \lambda}{\mu} \\
        P'(x) &= \frac{\mu \pi_0}{(\lambda x^2 + 2 \lambda x - \mu)^2} (2 \lambda x + 2 \lambda) \\
        \implies P'(1) &= \frac{\mu \pi_0}{(3\lambda - \mu)^2} \cdot 4\lambda \\
        \intertext{Substituting \(\pi_0 = \frac{\mu - 3\lambda}{\mu}\), we get}
        P'(1) &= \frac{\mu - 3\lambda}{(3\lambda - \mu)^2} \cdot 4\lambda \\
        P'(1) &= \frac{4\lambda}{\mu - 3\lambda} \\
        \implies E(X) &= \frac{4\lambda}{\mu - 3\lambda}
    \end{align*}

    Obviously, this only exists when \(\mu - 3\lambda > 0 \implies \frac{\lambda}{\mu} < \frac{1}{3}\)
}

\item {
    Queueing Models

    A company has one 16-core machine, two 8-core machines and two 4-core machines. They want
    to use them as servers. The inter arrival time of queries is exponentially distributed
    with mean 0.1ms. They estimate that the time taken by a core per query
    would be exponentially distributed with mean time 3, 2, 4 milliseconds for the 16-core,
    8-core and 4-core machines respectively. They want to set up a simple static load 
    balancer in front of these machines, which will schedule the queries on a core of a 
    machine with a probability to assign the query to each core.

    Determine the probabilities by which the load balancer should schedule queries on each
    type of core to minimise the maximum expected waiting time for a query. Note that one 
    query would occupy the core on which it's running for the entire time it's running.

    \textbf{Answer}

    Basically, what we want to do here is to divide the incoming queries into 3 different
    types of cores (they are different since they have different service time distributions)
    We can tabulate the information as follows:

    \begin{center}
        \begin{tabular}{| c | c | c | c |}
            \hline
            Number of machines & Number of cores & Total number of cores & Mean service time(ms) \\
            \hline
            1 & 16 & 16 & 3 \\
            2 & 8 & 16 & 2 \\
            2 & 4 & 8 & 4 \\
            \hline
        \end{tabular}
    \end{center}

    Number the types of cores given in the above table as 1, 2, 3 and let \(p_1, p_2, p_3\)
    denote the probabilities by which a query will be sent to a core of that type by the load
    balancer.

    Given that the incoming queries form a Poisson process with parameter 
    \(\frac{1}{0.1 \text{ ms }} = 10 \text{ ms}^{-1}\) and the load balancer is decomposing
    this Poisson process into separate streams. Then, the queries going to cores of type
    1, 2, 3 will form a Poisson process with parameters \(10p_1, 10p_2, 10p_3\) respectively
    and \(16p_1 + 16p_2 + 8p_3 = 1\) since each query will be routed to one of the given
    cores.

    We now model each core as a M/M/1 queue. Generically, let's take the arrival process
    parameter as \(\lambda\) and the service time parameter as \(\mu\).

    Let \(W\) be the waiting time for a query, then \(P(W < 0) = 0\) and \(P(W = 0) = \rho\)
    where \(\rho = \frac{\lambda}{\mu}\).

    For \(W > 0\), there has to be atleast one person in the system. Assuming there are n
    people in the system, we have \(W = \widetilde{S_1} + S_2 + \cdots + S_n\), where 
    \(\widetilde{S_1}\) is the time left for query that is already running, and \(S_2 \cdots S_n\)
    are the running times for the remaining queued queries.

    Since service times are exponentially distributed, which has memoryless property,
    \(\widetilde{S_1}\) is also exponentially distributed with parameter \(\mu\)

    Therefore, \(W / N = n \sim Gamma(n, \mu)\)  since it is a sum of \(n\) independent
    exponentially distributed random variables with parameter \(\mu\).

    For \(t > 0\),
    \begin{align*}
        P(W \leq t) &= \sum_{n=1}^{\infty} P(W \leq t / N = n) P(N = n) \\
                    &= 1 - \rho + \sum_{n=1}^{\infty} \int_0^{t} \frac{\mu^n x^{n-1}e^{-\mu x}}{(n - 1)!}\,dx\,
                        (1 - \rho) \rho^n \\
        \intertext{Now, taking summation inside the integral, we get}
        P(W \leq t) &= 1 - \rho + \int_0^t \sum_{n=1}^{\infty} \frac{(\mu x \rho)^{n-1}}{(n - 1)!} e^{-\mu x}\,dx\,(1-\rho)\mu\rho \\
                    &= 1 - \rho + \int_0^t \mu \rho e^{\mu x \rho} e^{-\mu x} \,dx\, (1 -\rho) \\
                    &= 1 - \rho + \mu \rho (1 - \rho) \frac{e^{-\mu(1-\rho)t} - 1}{\mu(\rho - 1)} \\
                    &= 1 - \rho + \rho (1 - e^{-(\mu - \lambda)t}) \\
                    &= 1 - \rho e^{-(\mu - \lambda)t}
    \end{align*}

    Therefore, the CDF of \(W\) in the steady state is given by
    \[
        P(W \leq t) = \begin{cases}
                        0 & t < 0 \\
                        1 - \rho & t = 0 \\
                        1 - \rho e^{-(\mu - \lambda)t} & 0 < t < \infty \\
                      \end{cases}
    \]
    where \(\rho = \frac{\lambda}{\mu}\) and the steady state solution is only possible
    when \(\rho < 1 \implies \lambda < \mu\)

    Then the pdf of \(W\) is given by \(f_W(t) = \rho(\mu - \lambda)e^{-(\mu - \lambda)t}\) when
    \(0 < t < \infty\) and 0 otherwise.
    \begin{align*}
        E(W) &= \int_0^\infty t \rho(\mu - \lambda)e^{-(\mu - \lambda)t} \,dt \\
             &= \frac{\rho}{\mu - \lambda} \int_0^\infty e^{-(\mu - \lambda)t} (\mu - \lambda) t \,d[(\mu - \lambda)t] \\
        \intertext{Since \(\mu - \lambda > 0\), \((\mu - \lambda)t \to \infty\) as \(t \to \infty\)}
        E(W) &= \frac{\rho}{\mu - \lambda} \int_0^\infty te^{-t}\,dt \\
             &= \frac{\rho}{\mu - \lambda} \\
             &= \frac{\lambda}{\mu^2 - \lambda\mu} \tag*{\(\left(\text{Since } \rho = \frac{\lambda}{\mu}\right)\)}
    \end{align*}

    Now substituting the values of \(\mu\) as \(\frac{1}{3}, \frac{1}{2}, \frac{1}{4}\) for
    cores 1, 2, 3 respectively and also using the query process parameters as calculated above,
    we get the expected waiting times for cores 1, 2, 3 respectively as:
    \[
        \frac{90p_1}{1 - 30p_1}, \frac{40p_2}{1 - 20p_2}, \frac{160p_3}{1 - 40p_3}
    \]
    
    We also need to have \(\rho < 1\) for all the cores, i.e. \(\frac{10p_i}{\mu_i} < 1\) 
    for all the cores, which gives \(p_1 < \frac{1}{30}, p_2 < \frac{1}{20}, p_3 < \frac{1}{40}\)

    So our problem reduces to the following optimisation problem:
    \begin{align*}
        \text{Minimise } &max\left\{\frac{90p_1}{1 - 30p_1}, \frac{40p_2}{1 - 20p_2}, \frac{160p_3}{1 - 40p_3}\right\} \\
        \text{In the domain } &16p_1 + 16p_2 + 8p_3 = 1, p_1 < \frac{1}{30}, p_2 < \frac{1}{20}, p_3 < \frac{1}{40}
    \end{align*}

    Solving the equations with the aid of computational tools available, we find that the minimum expected waiting time
    is approximately 4.77 ms, when \(p_1 \approx 0.020, p_2 \approx 0.035, p_3 \approx 0.013\)

    So, we have the probabilities by which the load balancer should send the queries to
    cores of type 1 as \(16p_1 \approx 0.327\), cores of type 2 as \(16p_2 \approx 0.564\)
    and cores of type 3 as \(8p_3 \approx 0.109\) for minimisation of the maximum expected
    waiting time for each query.
}

\item {
    Queueing Models

    A restaurant on a very busy main road has a parking capacity of 6 cars. People whose car
    does not get a parking space can't go to eat in the restaurant due to the busyness of
    the road and the unavailability of any other parking nearby. People in a car are binomially
    distributed as B(5, 0.5) (0 included since some people just park their car and not eat). 
    Arrival of cars follows a Poisson process with average inter arrival time 10 minutes.
    Time spent in the restaurant by one cars passengers is exponentially distributed with 
    average as 40 minutes.

    Draw the state diagram of the underlying process, and derive the steady state equations.
    What is the expected number of people in the restaurant in steady state?

    \textbf{Answer}

    By the description, we can observe the cars follow a M/M/6/6 queue.
    Let \(\lambda\) be the parameter of the arrival process and \(\mu\) be the parameter 
    of the service time exponential distribution. Then, by the given information,
    \(\lambda = \frac{1}{10} \text{ min}^{-1}\) and \(\mu = \frac{1}{40} \text{ min}^{-1}\)
    
    The state diagram of the underlying birth death process is then given by:

    \begin{tikzpicture}
        \node[state] (0) {0};
        \node[state, right=of 0] (1) {1};
        \node[state, right=of 1] (2) {2};
        \node[state, right=of 2] (3) {3};
        \node[state, right=of 3] (4) {4};
        \node[state, right=of 4] (5) {5};
        \node[state, right=of 5] (6) {6};

        \draw[every loop]
            (0) edge[bend left, auto=left] node {\(\lambda\)} (1)
            (1) edge[bend left, auto=left] node {\(\lambda\)} (2)
            (2) edge[bend left, auto=left] node {\(\lambda\)} (3)
            (3) edge[bend left, auto=left] node {\(\lambda\)} (4)
            (4) edge[bend left, auto=left] node {\(\lambda\)} (5)
            (5) edge[bend left, auto=left] node {\(\lambda\)} (6)
            
            (6) edge[bend left, auto=left] node{\(6\mu\)} (5)
            (5) edge[bend left, auto=left] node{\(5\mu\)} (4)
            (4) edge[bend left, auto=left] node{\(4\mu\)} (3)
            (3) edge[bend left, auto=left] node{\(3\mu\)} (2)
            (2) edge[bend left, auto=left] node{\(2\mu\)} (1)
            (1) edge[bend left, auto=left] node{\(\mu\)} (0);
    \end{tikzpicture}

    Here the numbers in each state indicate the number of cars parked in the parking lot.
    Let \(\{X_t \,|\, t \in \mathbb{R}\}\) be the underlying stochastic process where \(X_t\)
    denotes the number of cars parked in the parking lot at time \(t\).

    Define \(\pi_n = \lim_{t \to \infty} \text{Prob}(X_t = n)\). Then using Kolmogorov, forward
    equations and letting \(t \to \infty\), we get:
    \begin{align*}
        0 &= -\lambda\pi_0 + \mu\pi_1 \\
        0 &= \lambda \pi_{i-1} + (i+1)\mu \pi_{i+1} - (\lambda + i \mu) \pi_i, 1 \leq i \leq 5 \\
        0 &= \lambda \pi_5 - 5\mu \pi_6 \\
    \end{align*}

    From the first equation, we get \(\pi_1 = \frac{\lambda}{\mu}\pi_0\).
    For \(i = 1\):
    \begin{align*}
        0 &= \lambda \pi_{0} + 2\mu \pi_{2} - (\lambda + \mu) \pi_1 \\
        \implies 0 &= \lambda \pi_{0} + 2\mu \pi_{2} - (\lambda + \mu) \frac{\lambda}{\mu}\pi_0 \\
        \implies 2\mu \pi_2 &= \frac{\lambda^2}{\mu} \pi_0 \\
        \implies \pi_2 &= \frac{\lambda^2}{2\mu^2} \pi_0
    \end{align*}

    By induction, we can show that
    \[\pi_i = \frac{\lambda^i}{i!\,\mu^i} \pi_0,\, 1 \leq i \leq 5\]

    And then using the last equation, we get:
    \begin{align*}
        0 &= \lambda \pi_5 - 5\mu \pi_6 \\
        \implies 0 &= \lambda \frac{\lambda^5}{5!\,\mu^5} \pi_0 - 6\mu \pi_6 \\
        \implies 6\mu \pi_6 &= \frac{\lambda^6}{5!\mu^5} \pi_0 \\
        \implies \pi_6 &= \frac{\lambda^6}{6!\mu^6} \pi_0
    \end{align*}

    Define \(\rho = \frac{\lambda}{\mu}\)
    Using the fact that  the total probability must be 1, which is also called the
    normalising condition, we get:
    \begin{align*}
        \sum_{n = 0}^{6} \frac{\rho^n}{n!} \pi_0 &= 1 \\
        \implies \pi_0 &= \frac{1}{\sum_{n = 0}^{6} \frac{\rho^n}{n!}}
    \end{align*}

    Let \(X\) be the random variable denoting the number of cars in the parking lot
    at steady state. Then,
    \begin{align*}
        E(X) &= \sum_{n = 0}^{6} n \cdot \frac{\rho^n}{n!} \pi_0 \\
             &= \sum_{n = 0}^{6} n \cdot \frac{\frac{\rho^n}{n!}}{\sum_{n = 0}^{6} \frac{\rho^n}{n!}} \\
             &= \rho \cdot \frac{\sum_{n=1}^{6} \frac{\rho^{n-1}}{(n-1)!}}{\sum_{n = 0}^{6} \frac{\rho^n}{n!}} \\
             &= \rho \cdot \frac{\sum_{n=0}^{5} \frac{\rho^{n}}{n!}}{\sum_{n = 0}^{6} \frac{\rho^n}{n!}} \\
             &= \rho \left(1 - \frac{\frac{\rho^6}{6!}}
                                    {\sum_{n = 0}^{6} \frac{\rho^n}{n!}}\right)        
    \end{align*}

    Using \(\lambda = \frac{1}{10}, \mu = \frac{1}{40}\) we get \(\rho = 4\). Then we can
    solve the solution using a calculator and get \(E(X) \approx 3.531\)

    Now, we have the expected number of cars in steady state, but we need to calculate the
    expected number of customers in the restaurant in steady state.

    Let \(N\) be the random variable denoting the number of customers in the restaurant in
    steady state. Let \(N_i\) be the number of passengers in car \(i\), then we have 
    \(N = \sum_{i = 0}^{X} N_i\), where \(X\) is the random variable denoting the number of cars
    in steady state. We know that \(N_i\)'s are iid random variables which 
    are distributed as \(B(5, 0.5)\) from which we get \(E(N_i) = 5 \times 0.5 = 2.5\)

    Using Wald's equation since \(N_i\)'s are iid and also independent from \(X\), we get
    \begin{align*}
        E(N) &= E(N_1) \times E(X) \\
             &\approx 2.5 \times 3.531 \\
             &= 8.828
    \end{align*}

    Therefore, the expected number of customers in the restaurant in steady state is
    approximately 9.
}

\end{enumerate}

\end{document}
